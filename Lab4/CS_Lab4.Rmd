---
title: "Computer Lab 4 Computational Statistics"
author: "Phillip Hölscher"
date: "8 2 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# libraries used in this lab
library(ggplot2)
```


# Question 1: Computations with Metropolis-Hastings

Consider the following probability density function:

$$f(x)\ \alpha \ x^5\  e^{-x},\ x>0$$

You can see that the distribution is known up to some constant of proportionality. If you are interested (NOT part of the Lab) this constant can be found by applying integration by parts multiple times and equals 120.

##1. Use Metropolis–Hastings algorithm 
to generate samples from this distribution by using proposal distribution as log–normal $\rm LN(X_t,1)$, take some starting point. Plot the chain you obtained as a time series plot. What can you guess about the convergence of the chain? If there is a burn–in period, what can be the size of this period?

```{r}
#pdf -  probability density function 
pdf = function(x){
  if(x<= 0){
    stop("x needs to be bigger than 0")
  }
  # stopifnot(x>0) # check - x >0
  x^5 * exp(-x)
}
```


```{r}
# Metropolis-Hastings Sampler
set.seed(12345)
# Initilize chain to X_0, t=0
t = 1
t_max = 1000
i = 1 # interations
x_0 = c(1) # starting point
x_t = rep(x_0,t_max) # vector to save y or x_t
while (t < t_max) {
  y = rlnorm(n = 1,meanlog = x_t[i],sdlog = 1)
  u = runif(1,0,1)
  q_x_t = dlnorm(x =x_t[i],meanlog = y,sdlog = 1) # take the density
  q_y  = dlnorm(x = y, meanlog =  x_t[i], sdlog = 1)
  alpha = ((pdf(y) * q_x_t)/ (pdf(x_t[i])*q_y))
  if(u < alpha){
    x_t[i+1] = y
  } else{
    #ii = i -1 # change this to x_t (don´t work with x_t+1 - so use value x_t-1)
    x_t[i+1] = x_t[i] 
  }
  i = i +1
  t = t+1
}

# # code from the lecture slide 14
# f.MCMC.MH<-function(nstep,X0,props){
#     vN<-1:nstep
#     vX<-rep(X0,nstep);
#     for (i in 2:nstep){
# 	X<-vX[i-1]
# 	Y<-rnorm(1,mean=X,sd=props)
# 	u<-runif(1)
# 	a<-min(c(1,(dnorm(Y)*dnorm(X,mean=Y,sd=props))/(dnorm(X)*dnorm(Y,mean=X,sd=props))))
# 	if (u <=a){
# 	  vX[i]<-Y
# 	  }
# 	else{
# 	  vX[i]<-X
# 	  }    
#     }
#     plot(vN,vX,pch=19,cex=0.3,col="black",xlab="t",ylab="X(t)",main="",ylim=c(min(X0-0.5,-5),max(5,X0+0.5)))
#     abline(h=0)
#     abline(h=1.96)
#     abline(h=-1.96)
# }

```

```{r}
# create time series plot 
# create plot data
plot_data_1_1 = data.frame(x_t)

ggplot(data = plot_data_1_1, aes(x = 1:length(x_t), y = x_t)) +
  geom_line() + ggtitle("Metropolis-Hastings Sampler") + ylab("x(t)") + xlab("i")

```

2. Perform Step 1 by using the chi–square distribution $\chi^2$ $(\lfloor X_t + 1\rfloor)$ as a proposal distribution, where$\lfloor x \rfloor$ is the floor function, meaning the integer part of x for positive x, i.e.$\lfloor 2.95 \rfloor = 2$

# Question 2: Gibbs sampling

A concentration of a certain chemical was measured in a water sample, and the result was stored in the data *chemical.RData* having the following variables:
- \X: day of the measurement
- \Y: measured concentration of the chemical.

The instrument used to measure the concentration had certain accuracy; this is why the mea-
surements can be treated as noisy. Your purpose is to restore the expected concentration values.

## 1. Import the data to R and plot the dependence of Y on X. 
What kind of model is reasonable to use here?

```{r}
# set working directory
# import the data
load("chemical.RData")
# create table for plot
data = data.frame("X" = X,
                  "Y" = Y)
# plot dependence of Y on X
X_Y_dependence = ggplot(data, aes(x = X, y = Y)) + 
  geom_point()+
  ggtitle("Dependence of Y on X") + 
  geom_smooth()
X_Y_dependence
```


2. A researcher has decided to use the following (random???walk) Bayesian model (n=number of observations, $\vec{\mu} = (\mu_1, ..., \mu_n)$ are unknown parameters):
$$ Y_i  = \mathcal{N}(\mu, \sigma = 0.2), i= 1,...,n$$
where the prior is
$$ p(\mu_1) = 1  $$
$$ p(\mu_{i+1} \mid \mu_i) = \mathcal{N}(\mu_i , 0.2) i = 1,...,n1  $$

Present the formulae showing the likelihood $p(\vec{Y_i}\mid \vec{\mu})$ and the prior $p(\vec{\mu})$. Hint: a chain rule can be used here $p(\vec{\mu}) = p(\mu_1) p(\mu_2 \mid \mu_1)p(\mu_3 \mid \mu_2) ... p(\mu_n \mid \mu_{n1}) $.




